[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How we built a Pragmatic Machine Learning Pipeline to Identify Fake Flysheets as part of a Library Workflow",
    "section": "",
    "text": "1 Preface\nIn 1979, two Swedish bands released albums. One of these albums, Voulez-Vous, was recorded by ABBA in Polar Studios, at its time, one of the most advanced — and expensive — recording studios in the world. The other album, We're Only in it for the Drugs, by the Swedish Punk band Ebba Grön was recorded on a mobile 8-channel mixer in a closed industrial office. Both these albums are great, but the resources available to create them were vastly different.\nThere is a prevalent perception that machine learning is only for those with deep pockets. For example, in 2022, Google created a new Language Model, PaLM, but the cost of training such a model is estimated to be between $9M to $23M. As a result, discussions of machine learning often focus on large tech companies because they are deemed to be the only ones with the finances to develop and use this technology.\nMachine learning, and in particular a branch of machine learning called deep learning, has dramatically impacted a range of domains over the past ten years. There has been a growing interest in using machine learning in gallery, library, archive and museum (GLAM) institutions. A barrier to further adoption of machine learning in the GLAM sector is the perception that it requires extensive technical skill, data, computing power and other resources.\nIn this book, we want to show that just as Ebba Grön was able to record a seminal album without the resources available to ABBA, GLAM institutions can use machine learning for practical work without the resources of Google."
  },
  {
    "objectID": "intro.html#whats-covered",
    "href": "intro.html#whats-covered",
    "title": "2  Introduction",
    "section": "2.1 What's covered",
    "text": "2.1 What's covered\nThe book covers a few key areas:\n\nthe motivation of the project\nthe various stages in the pipeline:\n\ndata collection\ndata versioning\nmodel development\nmodel evaluation\ntracking results\ndeployment\nupdating the model\n\n\nAll of these are discussed with a particular focus on how they fit into existing workflows and how they can be done pragmatically."
  },
  {
    "objectID": "intro.html#what-isnt-covered",
    "href": "intro.html#what-isnt-covered",
    "title": "2  Introduction",
    "section": "2.2 What isn't covered",
    "text": "2.2 What isn't covered\nThis isn't a guide on how to do machine learning itself. Although some of the sections touch upon some of these considerations, this book won't explain everything thoroughly. If you want a more conceptual introduction to machine learning, we recommend https://carpentries-incubator.github.io/machine-learning-librarians-archivists. For a hands-on introduction, we recommended https://course.fast.ai/."
  },
  {
    "objectID": "intro.html#who-is-this-for",
    "href": "intro.html#who-is-this-for",
    "title": "2  Introduction",
    "section": "2.3 Who is this for?",
    "text": "2.3 Who is this for?\nWe hope that this book will be useful for other GLAM institutions wanting to embark on machine learning projects. In particular, we are keen to address ‘business as usual’ use cases of machine learning in GLAM institutions as we believe there is massive scope for using machine learning methods across a range of ‘mundane’ GLAM activities."
  },
  {
    "objectID": "intro.html#practical-machine-learning-projects",
    "href": "intro.html#practical-machine-learning-projects",
    "title": "2  Introduction",
    "section": "2.4 Practical machine learning projects?",
    "text": "2.4 Practical machine learning projects?\nA report has suggested that 85% of AI projects “ultimately fail to deliver on their intended promises to business”11. This stat might be a little dubious, but there is some truth to the claim that machine learning projects can be challenging. By a “machine learning project,” we mean a project that intends to use machine learning to solve a problem. Part of this process could include creating new machine learning models, but it could also rely on existing models.\nWe use the term “practical” here to try and distinguish this type of project slightly from other projects which may involve using machine learning in a GLAM setting but are more focused on research. The outputs of these projects may need to integrate less tightly into existing business workflows and processes. This isn’t to say that these kinds of projects won’t have their own challenges, but they will likely be different. ‘Practical’ machine learning projects could include workflows and processes that end users don’t see and are likely to be the kinds of projects that may struggle to get external funding.\n\n\n\n\nAmeisen, Emmanuel. Building Machine Learning Powered Applications: Going from Idea to Product. \" O’Reilly Media, Inc.\", 2020.\n\n\nDarby, Andrew, Catherine Nicole Coleman, Claudia Engel, Daniel Alexander van Strien, Michael Trizna, and Zachary Painter. “AI Training Resources for GLAM: A Snapshot.” ArXiv abs/2205.04738 (2022).\n\n\nHuyen, Chip. Designing Machine Learning Systems. \" O’Reilly Media, Inc.\", 2022.\n\n\nLakshmanan, Valliappa, Sara Robinson, and Michael Munn. Machine Learning Design Patterns. O’Reilly Media, 2020.\n\n\nvan Strien, Daniel, Mark Bell, Nora Rose McGregor, and Michael Trizna. “An Introduction to AI for GLAM.” In Proceedings of the Second Teaching Machine Learning and Artificial Intelligence Workshop, edited by Katherine M. Kinnaird, Peter Steinbach, and Oliver Guhr, 170:20–24. Proceedings of Machine Learning Research. PMLR, 2022. https://proceedings.mlr.press/v170/strien22a.html."
  },
  {
    "objectID": "business_problem.html#do-you-need-machine-learning-at-all",
    "href": "business_problem.html#do-you-need-machine-learning-at-all",
    "title": "3  Approaching ML projects: defining the ‘problem’",
    "section": "3.1 Do you need machine learning at all?",
    "text": "3.1 Do you need machine learning at all?\nOnce you have decided what problem you are trying to tackle you should also step back and ask whether machine learning is the only – or best – answer. Some other options that may make more sense:\n\ndo the task manually\ndo the task manually as part of an internal effort\ndo the task through public facing crowdsourcing\ndo the task in a computational way without machine learning\ndon’t do the task at all\n\nWhilst machine learning can be a very powerful tool, machine learning projects are usually not ‘quick’ or ‘easy’ wins.\nIf you have an existing process you are trying to integrate machine learning into you can use this existing approach as a comparison to any new approaches developed. This can be very useful for assessing the extent to which machine learning is practical in the setting you are using it."
  },
  {
    "objectID": "business_problem.html#defining-the-business-problem-for-hertiage-made-digital",
    "href": "business_problem.html#defining-the-business-problem-for-hertiage-made-digital",
    "title": "3  Approaching ML projects: defining the ‘problem’",
    "section": "3.2 Defining the ‘business’ problem for Hertiage Made Digital",
    "text": "3.2 Defining the ‘business’ problem for Hertiage Made Digital\nThe British Library has a vast amount of ‘legacy’ digitised content stored on network drives. The Heritage Made Digital (HMD) team has been tasked with making these available for users online. We have multiple terabytes of image files, mostly containing photos or scans of the items in our physical collection. One of the viewing platforms that we have made our manuscript image sets available on over the years is our Digitised Manuscripts (DM) website so that people can look at and study the digitised versions of the physical objects without needing to come into one of our buildings in St Pancras or Boston Spa.\nThe thing is, DM was never designed to be our primary, strategic, good-for-all-occasions image viewer. It was built for a specific project that wanted to publish a limited number of similarly constructed manuscripts. Namely the Greek Manuscripts project.\n\n\n\n\n\n\n\n(a) Digitsed manuscript platform\n\n\n\n\nFigure 3.1: Digitised manuscripts platform for 1784.a.13.1\n\n\nThe physical structure of the items in the Greek Manuscripts project are largely uniform. They are bound codices (of varying sizes and material types) and to digitise these all you need to do is take a photo of the front cover, front flysheets, folios, end flysheets, back cover and spine. That’s six different types of images. Or, in terms of our standardised filenames (ignoring the r and v we put at the end of filenames for each recto and verso shot, for the sake of simplicity):\n\nfblef - front cover\nfs001 - front flysheet\nf001 - folio\nfse001 - end flysheet\nfbrig - back cover\nfbspi - spine\n\nAnd so DM was built to accommodate these and only these options, with no room for something physically unusual that didn’t fit this standard.\nRegardless of this shortcoming, DM was pretty good, and better than anything else we had at the time, so other projects started using it to publish their manuscript material. This is where the problem arose.\nThe material in these other projects wasn’t as uniform as the items in the Greek Manuscripts project, and sometimes we wanted to take images of fore-edges, or clasps, or bags, or anything else interesting or integral to the physical item. The only way to publish these images on DM was to assign them one of the six standard filenames, and the agreed approach was to call them an end flysheet, and give it a number at the end of the sequence. In other words, if you already have two end flysheets, the image filename of the fore-edge will be called fse003. A ‘fake’ flysheet.\nThis worked fine for years, but we are now at the point of moving on from Digitised Manuscripts and we are beginning to migrate all our digital content into our new preservation and access system, allowing people to view and download images from our IIIF-compatible viewer, the Universal Viewer.\nThis new system allows us to display whatever images we want, in any sequence order, and label them however we want, regardless of the filename. This gives us an opportunity to correctly relabel images that were published to DM as ‘fake’ flysheets, so that users can better understand the digital objects we’re presenting to them.\n\n\n\n\n\n\n\n(a) Digitsed manuscript platform showing page types being derived from the filename\n\n\n\n\nFigure 3.2: Digitised manuscripts platform with pages types\n\n\nUnfortunately, however, we kept little to no records of which images we renamed as ‘fake’ flysheets. There are potentially hundreds or thousands of these scattered throughout the tens of thousands of items we’ve digitised. Rather than manually checking every image as we migrate it to the new system we thought it might be more efficient and sustainable to use an automated or semi-automated computational method to do this task, or at least get one to help us to do it.\nIt’s worth saying that even though we may have initially dreamed of a magic program to solve all our problems - identifying all our ‘fake’ flysheets with a click of a button and relabelling them for us - we very quickly downgraded our expectations. The main reason we couldn’t put our faith in a totally automated solution is because we know our collection (or at least we know how physically varied it can be), how big it is, and how we don’t really know everything about it. Given the myriad of items we’ve digitised over the last 20+ years, we were pretty sure a tool wouldn’t be able to pick out all the subtle differences in images with 100% accuracy - a task even trained humans find difficult to do. This turned out to be a fairly sensible assumption and what we actually needed was a tool that could be used as part of our existing migration workflow, confirming the large majority of images as real flysheets whilst flagging up images it suspected were fakes, for a human to then manually confirm or deny by looking at the image.\nSo the task became one to create a tool as quickly and easily as possible, that was simple to use and maintain, that fit into our existing workflow, gave us accurate results, and we felt confident enough to trust. Because we lacked the required software development skills within the team, we turned to colleagues with expertise in machine learning and set up a meeting to outline our requirements; so the concept for flyswot was born.\nA bonus to this whole approach was that we could work more closely with colleagues we don’t necessarily engage with on a day-to-day basis. It also brought a subset of our team closer together in pursuit of a new extracurricular activity, and we’d all be able to learn about new tools and technologies."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This next section deals with data. Whilst data has been recognised as a vital component of machine learning it hasn’t always been given the same attention as other parts of the pipeline. Recently a greater emphasis has been given to data.\nWe only scratch the surface of some of the issues related to data. We focus in particular on:\n\nannotating our data\nusing data for evaluation"
  },
  {
    "objectID": "creating-training-data-annotation.html#what-annotation-tool-did-we-use",
    "href": "creating-training-data-annotation.html#what-annotation-tool-did-we-use",
    "title": "4  Creating training data: annotation",
    "section": "4.1 What annotation tool did we use?",
    "text": "4.1 What annotation tool did we use?\nMicrosoft️ Windows File Explorer! More specifically, we create a folder for each label in our training data and add images that belong to that label so it looks something like this:\n\nIn our case, each image should belong to only one category e.g. CONTAINER so any images matching this category should go in this folder.\n\n4.1.1 Why did we use this approach?\nOrganising data and the associated labels in this folder is a fairly standard way of distributing machine learning datasets. This format is supported in various downstream machine-learning frameworks.2\nHowever, whilst this is often used as a ‘distribution’ format, it is perhaps slightly unusual to use it to collect annotations. The reason we chose to use this approach:\n\nminimal setup costs (both in terms of time, effort and cost);\nusing folders to organise files is familiar to most people already;\nit more closely fits into existing workflows.\n\nThis final point is worth additional thought when trying to do a practical ML project within a GLAM setting. Annotating data can be time-consuming and, depending on the task, quite boring. Making annotation a brand new ‘separate’ activity removes it from existing work and may lead to fewer people being able to contribute to this activity. In our particular situation, since members of the HMD team were already working through image files in preparation for ingest into their digital preservation store LibSafe, taking the extra step of putting any ‘fake flysheet’ they spotted into the ‘training folder’ wasn’t too far outside of their work process. If we had collected annotations through a more formal platform, the annotation process would be a separate activity, likely done through a web interface. Whilst this interface might be faster if the sole activity is doing annotations, a more informal approach may work better if the annotation process is to fit into an existing workflow.\nHow annotations are collected is an area of the ML pipeline where GLAMs may diverge from the more traditional workflows used in industry/commercial settings. The model of ‘farming out’ annotating as a full-time activity will usually not be financially possible (and there might be other professional or ethical concerns about doing so). Beyond this, it might also not make sense to try and replicate a ‘full-time’ annotation approach when the ML project has limited resources available. Instead, in these settings, thinking about how annotations could be collected more easily as part of a workflow might be sensible.\n\n\n4.1.2 Why shouldn’t you use this approach?\nThere are limitations to this kind of approach that will make it unsuitable for some tasks. The first obvious limitation is that we can only annotate for performing classification using this approach, as the label for each image comes from the folder in which it sits. It could be a better approach for multi-label datasets, i.e. where each photo can have one, multiple or no associated labels. Beyond the tasks that this kind of approach can’t easily support, some other limitations include the following:\n\nminimal or no provenance information about who did the annotation, when it was done etc.\nno easy way of managing annotations as ‘tasks’ for different team members\nno easy way of generating inter-annotator agreement scores (measures of how much different people completing the annotations agree with each other).\n\nMost of these challenges stem from the fact that a file system isn’t intended for this kind of task. Although file systems usually record some information about changes to folders etc., this is not always very granular.\nThe final limitation of not being able to generate inter-annotator agreement scores are potentially very important for some projects. For example, a project aiming to apply metadata labels automatically to images or documents should carefully assess how much annotators agree with each other by getting multiple people to label the same item. If there is a lot of disagreement between annotators, then the labels might be ambiguous, or the task of giving the correct label might be a challenging one requiring expert knowledge. In these scenarios, it is worth the extra effort of setting up a proper annotation tool. Some options for these annotations platforms include:\n\nLabelstudio\nLabelme\nProdigy\nCVAT\nZooniverse\n\nAn exploding number of start-ups and companies are promising to have solved data labeling and curation once and for all, so this list is not comprehensive. Each tool or platform will have benefits and limitations that should be considered. Sharing insights into using these platforms and tools for generating training data is a potential area for further GLAM collaboration, particularly as setting up some annotation tasks is a non-trivial process. There are substantial advantages to using an open-source tool, but this should not be the sole consideration.\n\n\n\n\nLee, B., Jaime Mears, Eileen Jakeway, Meghan M. Ferriter, Chris Adams, Nathan Yarasavage, Deborah Thomas, Kate Zwaard, and Daniel S. Weld. “The Newspaper Navigator Dataset: Extracting Headlines and Visual Content from 16 Million Historic Newspaper Pages in Chronicling America.” Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 2020."
  },
  {
    "objectID": "creating-training-data-continued.html",
    "href": "creating-training-data-continued.html",
    "title": "5  Creating training data: other considerations",
    "section": "",
    "text": "In the previous chapter, we discussed different routes to annotating our data and the approach we ended up taking. This section will briefly discuss other data aspects we found necessary to consider as part of our project.\n\n5.0.1 Finding existing datasets?\nOne reasonable question you may have had whilst reading the previous chapter is: why not use some existing data? Unfortunately, we often won’t find a prepared dataset ready for use. In some forthcoming ‘bonus’ chapters, we’ll discuss ways in which you could leverage the wealth of unlabeled data created by GLAM institutions. However, this is a more advanced topic we explored later in our project, so we won’t cover it here.\nWith the growing interest in using machine learning in GLAMs, there are an increasing number of datasets you might be able to use as a starting point. Good places to look for these datasets include:\n\nHugging Face datasets hub (in particular, the BigLAM organization is explicitly trying to make more GLAM data available;e for machine learning.\nZenodo\n\n\n\n5.0.2 How much data do we need?\nTODO\n\n\n5.0.3 Data tools\nTODO\n\n\n5.0.4 How to ensure we’re using our annotated data carefully?\nWhilst creating our annotated data is one important step in a machine learning project we have to consider how to use this data carefully. The next chapter will discuss some of these considerations in a little more detail."
  },
  {
    "objectID": "a_note_on_notebooks.html#how-we-originally-executed-our-notebooks",
    "href": "a_note_on_notebooks.html#how-we-originally-executed-our-notebooks",
    "title": "6  A note on notebooks",
    "section": "6.1 How we (originally) executed our notebooks?",
    "text": "6.1 How we (originally) executed our notebooks?\nWhilst these notebooks weren’t designed to be run outside of our use case we did make some effort, in particular early on during our project to make our work slightly more “reproducible”. In particular, we used a tool called DVC to organize our notebooks into a ‘pipeline’ that could be executed from a single command. This meant that as we updated the data entering all the other steps we could easily update the later stages of the process which depend on this process. This made it easier in the early stages of this project to iterate on the process.\nWe’ll talk more about this aspect in a later chapter on tooling.\n\n\n\n\n\n\nWarning\n\n\n\nTrue reproducibility is not always that easy to achieve in machine learning. We usually think of computers as being fairly predictable when given the same input. For example the following python code,\n\n>>>grades = [88, 90, 80]\n>>>mean_grade = sum(grades)/len(grades)\n>>>print(mean_grade)\n86\nshouldn’t change. However, there is quite a bit of potential randomness in machine-learning models that can result in different outputs occurring for code that appears to be the same. There are ways of trying to get around this but we flag this to be clear about what we mean here.\n\n\nPipeline tools\n\n6.1.1 DVC\nWhat we might consider if we were starting this project now."
  }
]